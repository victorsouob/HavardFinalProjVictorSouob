---
title: "Untitled"
author: "victor souob"
date: "22/08/2020"
output:
  pdf_document: default
  word_document: default
---


HAVARDX FINAL PROJECT DATA SCIENCE
VICTOR SOUOB


DEMONSTRATION OF THE RMFT MARKETING MODEL BY STUDYING THE BEHAVIOR OF A BLOOD TRANSFUSION SERVICE CENTER DATA SET IN TAIWAN.




INTRODUCTION

In today business world, is important to know the tendencies of clients and customer’s, in order for companies or organization to invest accordantly. The regency, frequency, monetary and time (RFMT) method is an approach used to measure costumer's loyalty and segment customers into various group for future personalization services (Yeh, I-Cheng et al, 2008). The data set used for this project come from the UCI machine learning website (https://archive.ics.uci.edu/ml/datasets/Blood+Transfusion+Service+Center).  
The initial data set has 748 observations for 5 variables (we created a 6th variable for the purpose of this study). The 6 variables can be describes as follow: 
R (Regency - months since last donation)
F (Frequency - total number of donation)
M (Monetary - total blood donated in c.c.)
T (Time - months since first donation) 
A binary variable representing whether he/she donated blood in March 2007 (1 stand for donating blood; 0 stands for not donating blood).
A 6th variable was created (period) to calculate the difference between the first month of donation and the last month of donation.
The aim here is to create and compare 3 different machine learning models by their accuracy of predicting if a random person will be coming back (or not) to donate blood within a period of time. 
The 3 models used here was K nearest neighbor’s, logistic regression and decision trees.  By comparing the accuracy of each model will we decided witch model is best to use

2- METHOD AND ANALYSIS

-DATA CLEAN UP
We started by loading all needed libraries, renamed all columns, we checked for all missing values and finally adding a sixth variable inside our data frame (data_period). 



```{r}
# Let's start by downloading all needed Libraries
if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")
if(!require(data.table)) install.packages("data.table", repos = "http://cran.us.r-project.org")

library(tidyverse)
library(caret)
library(dplyr)
library(rpart)

# Download dataset from UCI respiratory website

data <- read.table("https://archive.ics.uci.edu/ml/machine-learning-databases/blood-transfusion/transfusion.data",fill = TRUE,header = TRUE,sep = ",")
head(data)

# Rename columns

tranfusion_dat_names <- c( "last_M_after_don", "Frequency_of_don", "Amount_blood","M_after_first_don", 
                           "Are_theyComingBack")



names(data) <- tranfusion_dat_names

head(data)

#-looking for missing values NA
sum(is.na(data))

# adding period to the dataset  by soubstrating first month of donation to last month

data_period<- data %>% mutate(period = c( M_after_first_don - last_M_after_don))
head(data_period)


```
- DATA EXPLORATION


```{r cars}

str(data_period)

# we are working with a data frame, has 748 rows and 6 variables
# The variable to predict "Are_theyComingBack"  is listed as numeric , and we will change it to a factor of 2 class(0,1)

```
The structure of the data_period that we are using for our study is a  data frame with 748 obs and 6 variables.
Also the variable to predict "Are_theyComingBack"  is listed as numeric , and we will change it to a factor of 2 class(0 = no,1 = yes). 
Now we will look at the summary of the data

```{r,ca }
# changing the variable to predict as a factor (0 = no, 1 = yes)
data_period$Are_theyComingBack <- as.factor(data_period$Are_theyComingBack)
# data summary
summary(data_period$period)
summary (data_period$Frequency_of_don)
summary(data_period$Amount_blood)

```
From top to bottom we have period, frequency of donation and amount of donation.
We oserved that it tooks only 19 months out  96 months to have half of the data collection.
The average total number of donation (frequency_of_don) was 5.515 and the maximum was 50.
the maximum amount of blood donated was 12500cc (cc is used here to quantify the amount of blood) , the minimum was 250 cc in less than a month.

DATA VISUALIZATION

Let's plot the frequeny of donation during the giving period

```{r pressure, echo=FALSE}
#  frequeny of donation during the giving period

data_period%>% select(Frequency_of_don,period) %>% 
  ggplot(aes(period,Frequency_of_don)) + geom_count()
```
We observed here that most of the blood donation seems to happen before the 50th month. We also observed that most of the blood donation decreased with time

```{r pressue, echo=FALSE}
## Blood vs frequency

data_period%>% select(Frequency_of_don,Amount_blood) %>% 
  ggplot(aes(Amount_blood ,Frequency_of_don)) + geom_count()
# when the frequency of donation increase , the amount of blood increase as well
# They are dependant.
```

Here we observe that when the total amount of donation increase , the amount of blood increase as well.
At the beginnig the frequency of donaation is hapenning at a higher rate but decreased with time.



```{r pressure1, echo=FALSE}
# lets observe the tendency of people to comeback  or not for blood donation in the datasets

ggplot(data_period, aes(x=Are_theyComingBack)) + geom_bar()
table(data_period$Are_theyComingBack)
## we see that proportion of peaople that are not coming back (570) are higher vs 178

```
In the data set a large proportion of people did not came back to donate .
In fact 570 did not came back compared to 178 (during that period) to donate blood.
This confirm we have only 2 classes (0,1) in our data set.

-INSIGHT
We have a supervised data to predict with known categories.
We also have a binary outcome (1 = blood donation, 0 = NO) to classify and predict for accuracy. 
Here we are trying to reproduce a RTMC model (Regency as months since last donation, Frequency as total number of donation, Monetary as total blood donated, Time as months since last donation) so we will purposely predict the outcome without identifying the best variable (forward selection, correlation etc…) that can generated the best accuracy .
KNN,logistic regression and decision trees seems to be good models to explore for accuracy.

-KNN model
We will used the KNN method to train the first model, test it and compute for the accuracy of the model.
The theorie here is to separated the supervised data (train and test set). Train the known supervised data (using crosss validation) and compare each point of the the test set , to the trained data. By looking at the mojority of classes (or the class with the higher average) surounding the variable to predict, We can then decide the ouput of a specific class. Prior to that , we will first find the best K (number of neighbor) that optimize the modeol.

-Logistic regresion
Logistic regression can also be use to classify sample. By knowing the classes we are trying to predict , the logistic regression fits an S shape (goes from 0 to 1) logistic function through the data (train_sample).  By looking at the probability ( if X is greater or smaller than 50 % on the S shape curve ), we can then classify an unknown variable. 

-Decision Trees
Are used in prediction problem where the outcome is a classification of data.
The decision Tree can de define Is a flow chart of Yes or No question , and the chalenge is to define an algorithm that used data to create tress with prediction at the end.


CREATE DATA PARTITION

We divided the data into a 2 sets train_sample( 80 %) and a test_sample(20%)

```{r, cars1}
set.seed(1)
data_period$Are_theyComingBack <- as.factor(data_period$Are_theyComingBack)
test_index <- createDataPartition(y = data_period$Are_theyComingBack, times = 1, p = 0.2, list = FALSE)
train_sample <- data_period [-test_index,]
test_sample <- data_period [test_index,]
```


```{r,cars2}

# lets check if we have the same probabiblity after partition

prop.table(table(train_sample$Are_theyComingBack))
prop.table(table(test_sample$Are_theyComingBack))
# The probability for train and test set are almost the same. 

```
The probability for train and test set are almost the same after oartition.


RESULTS

KNN model

```{r, cars3}
# lets find the best k for the smallest RMSE

set.seed(1)
ctrl <- trainControl(method="repeatedcv",repeats = 3)
knnFit <- train(Are_theyComingBack ~ ., data = train_sample, method = "knn", trControl = ctrl, preProcess = c("center","scale"),tuneLength = 20)
knnFit


```

```{r, pressure3, echo=FALSE}

#Plotting yields Number of Neighbours Vs accuracy (based on repeated cross validation)

plot(knnFit)






```
The final value used for the model was k = 13.


 
```{r, cars20}
# lets predict with the test set
y_hat_knn <- predict(knnFit,test_sample, type = "raw")


# lets find the accuracy

confusionMatrix(y_hat_knn,test_sample$Are_theyComingBack)$overall[["Accuracy"]]
## we have an accuracy of 0.8with KNN



```
KNN model has an accuracy of 0.8


LOGISTIC REGRESSION
 
```{r,cars7}

##### LOGISTIC REGRESSION
# lets fitt the model 

glmFit <- train(Are_theyComingBack ~ ., data = train_sample, method = "glm", family="binomial")
glmFit


# lets predict it using test set and  find the accuracy
y_hat_glmn<- predict(glmFit, test_sample, type = "raw")
y_hat_glmn
confusionMatrix(data = y_hat_glmn, reference = test_sample$Are_theyComingBack)$overall[1]
### final accuracy of our logisic regresion model is 0.76




```
The accuracy of the model with logistic regression is 0.76

DECISSION TREES
 
 
```{r, cars8}

#### DECISION TREES

# lets train the rpart and find the best cp
train_rpart <- train(Are_theyComingBack~ .,
                     method = "rpart",
                     tuneGrid = data.frame(cp = seq(0.0, 0.1, len = 25)),
        
                                  data = train_sample)
plot(train_rpart)

```



```{r, cars10}

## lets find the accuracy of the model
confusionMatrix(predict(train_rpart, test_sample), test_sample$Are_theyComingBack)$overall["Accuracy"]
### accuracy of 75 percent
```
With decision Tree we have an accuacy of 0.75

 
```{r, cars 11}

# lets look at some feature
train_rpart$finalModel

```

This is a summary description of our data with decision tree. For example, Here we observe that 9.5 months after the last donation, donors with an averall frequency lower than 4.5 did came back to donate blood. If the period of donation was more than 46.5 months donors with a frequency of donation higher than 4.5 will most likely donate. Also for a period of 46.5 months donors with a frequency of donation lower than 25 will not , most likely comeback. 


Findings
Knn with an accuracy of 0.8 is by far the highest of the 3 model we studied here, followed by Logistic regression (0.76) and decesion trees (0.75). Althoug The decision Tree give us a very detail summary of data compared to KNN and Logistic regression.
Overall the decison tree give us more information but with a lower accuracy compare to KNN.


CONCLUSION
In summary , we have tested 3 different model. KNN is the most accurate of the 3 models but it is less informative compare to Decison Tree. One of the limitation was the sample size . In futur works, we could replicate this study with a larger sample size and predict the accuracy. Also we knict our project from rmd to pdf , we relised a slighty change of our result for KNN in the rmd file and the PDF file. Having a good (bigger Ram) will probably help to eliminate those variation.

